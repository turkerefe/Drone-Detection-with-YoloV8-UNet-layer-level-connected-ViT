{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyN6136mw1TEvN87L06gxgcU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q4brssuNZher","executionInfo":{"status":"ok","timestamp":1753438435012,"user_tz":-180,"elapsed":3427,"user":{"displayName":"Türker Efe Soyutemiz","userId":"11416081914510237351"}},"outputId":"203d08b3-c585-4abd-acae-ee6801ab3f51"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import os\n","from tqdm.auto import tqdm\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import cv2, numpy as np, xml.etree.ElementTree as ET\n","import albumentations as A\n","\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# Hücre 2\n","import gdown\n","import shutil\n","def download_and_unpack(file_id, dest_folder):\n","    parent = os.path.dirname(dest_folder)\n","    os.makedirs(parent, exist_ok=True)\n","    zip_path = f\"{dest_folder}.zip\"\n","    gdown.download(id=file_id, output=zip_path, quiet=False)\n","    tmp_dir = dest_folder + '_tmp'\n","    os.makedirs(tmp_dir, exist_ok=True)\n","    get_ipython().system(f'unzip -q {zip_path} -d {tmp_dir}')\n","\n","    for root, dirs, files in os.walk(tmp_dir):\n","        for f in files:\n","            src = os.path.join(root, f)\n","            dst = os.path.join(dest_folder, f)\n","            os.makedirs(dest_folder, exist_ok=True)\n","            os.replace(src, dst)\n","    shutil.rmtree(tmp_dir)\n","\n","\n","train_id = '1lGumpFGdAvLWXhdLJCbkpv-Uo-t8fpHT'\n","test_id  = '1dJdUIqTfW7InN76Xb3flWVrVmcficP93'\n","\n","\n","download_and_unpack(train_id, '/content/drone_data/Drone_TrainSet')\n","download_and_unpack(train_id, '/content/drone_data/Drone_TrainSet_XMLs')\n","download_and_unpack(test_id,  '/content/drone_data/Drone_TestSet')\n","\n","print(\"Veri klasörleri:\", os.listdir('/content/drone_data'))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ld8ur8Jco38","executionInfo":{"status":"ok","timestamp":1753438592680,"user_tz":-180,"elapsed":156612,"user":{"displayName":"Türker Efe Soyutemiz","userId":"11416081914510237351"}},"outputId":"cc276b38-3626-4039-934f-3a0dc0925857"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From (original): https://drive.google.com/uc?id=1lGumpFGdAvLWXhdLJCbkpv-Uo-t8fpHT\n","From (redirected): https://drive.google.com/uc?id=1lGumpFGdAvLWXhdLJCbkpv-Uo-t8fpHT&confirm=t&uuid=1476c78a-e831-4046-8574-b3c009fc5c6e\n","To: /content/drone_data/Drone_TrainSet.zip\n","100%|██████████| 4.36G/4.36G [00:41<00:00, 105MB/s] \n","Downloading...\n","From (original): https://drive.google.com/uc?id=1lGumpFGdAvLWXhdLJCbkpv-Uo-t8fpHT\n","From (redirected): https://drive.google.com/uc?id=1lGumpFGdAvLWXhdLJCbkpv-Uo-t8fpHT&confirm=t&uuid=f720d130-7375-49fb-ba7d-b786f4f9fcfb\n","To: /content/drone_data/Drone_TrainSet_XMLs.zip\n","100%|██████████| 4.36G/4.36G [00:17<00:00, 253MB/s]\n","Downloading...\n","From (original): https://drive.google.com/uc?id=1dJdUIqTfW7InN76Xb3flWVrVmcficP93\n","From (redirected): https://drive.google.com/uc?id=1dJdUIqTfW7InN76Xb3flWVrVmcficP93&confirm=t&uuid=1375d04d-d4d1-46bd-b742-46d3e123a167\n","To: /content/drone_data/Drone_TestSet.zip\n","100%|██████████| 569M/569M [00:02<00:00, 193MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Veri klasörleri: ['Drone_TrainSet_XMLs.zip', 'Drone_TrainSet', 'Drone_TestSet', 'Drone_TrainSet_XMLs', 'Drone_TrainSet.zip', 'Drone_TestSet.zip']\n"]}]},{"cell_type":"code","source":["%pip install --upgrade ultralytics segmentation-models-pytorch timm albumentations xmltodict gdown tqdm matplotlib"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"5DtRrc8pcuPB","executionInfo":{"status":"ok","timestamp":1753411468694,"user_tz":-180,"elapsed":79639,"user":{"displayName":"Türker Efe Soyutemiz","userId":"11416081914510237351"}},"outputId":"ff4afe47-02df-487e-e35e-72ee9621b813"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.3.169-py3-none-any.whl.metadata (37 kB)\n","Collecting segmentation-models-pytorch\n","  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.17)\n","Collecting timm\n","  Downloading timm-1.0.19-py3-none-any.whl.metadata (60 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/60.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.8)\n","Collecting xmltodict\n","  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n","Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Collecting matplotlib\n","  Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.12.0.88)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.3.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.16.0)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.33.4)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.5.3)\n","Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.11.7)\n","Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.24)\n","Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.12.0.88)\n","Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (3.12.5)\n","Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (6.5.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2025.7.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (4.14.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (1.1.5)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.7.14)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n","Downloading ultralytics-8.3.169-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading timm-1.0.19-py3-none-any.whl (2.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n","Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m110.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m128.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n","Installing collected packages: xmltodict, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, matplotlib, nvidia-cusolver-cu12, ultralytics-thop, ultralytics, timm, segmentation-models-pytorch\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: matplotlib\n","    Found existing installation: matplotlib 3.10.0\n","    Uninstalling matplotlib-3.10.0:\n","      Successfully uninstalled matplotlib-3.10.0\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","  Attempting uninstall: timm\n","    Found existing installation: timm 1.0.17\n","    Uninstalling timm-1.0.17:\n","      Successfully uninstalled timm-1.0.17\n","Successfully installed matplotlib-3.10.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 segmentation-models-pytorch-0.5.0 timm-1.0.19 ultralytics-8.3.169 ultralytics-thop-2.0.14 xmltodict-0.14.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["matplotlib","mpl_toolkits"]},"id":"b390dc7f51294b8692df80777bccbe3d"}},"metadata":{}}]},{"cell_type":"code","source":["# Hücre 3: Dataset ve DataLoader\n","def xml_to_mask(xml_path, img_h, img_w):\n","    mask = np.zeros((img_h, img_w), dtype=np.uint8)\n","    if not os.path.isfile(xml_path):\n","        return mask\n","    tree = ET.parse(xml_path)\n","    root = tree.getroot()\n","    for obj in root.findall('object'):\n","        bbox = obj.find('bndbox')\n","        xmin, ymin = int(float(bbox.find('xmin').text)), int(float(bbox.find('ymin').text))\n","        xmax, ymax = int(float(bbox.find('xmax').text)), int(float(bbox.find('ymax').text))\n","        mask[ymin:ymax, xmin:xmax] = 1\n","    return mask\n","\n","class DroneDataset(Dataset):\n","    def __init__(self, img_dir, xml_dir, transform=None):\n","        self.img_paths = sorted([os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.endswith('.jpg')])\n","        self.xml_dir = xml_dir\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.img_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.img_paths[idx]\n","        img = cv2.imread(img_path)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        h, w = img.shape[:2]\n","        xml_path = os.path.join(self.xml_dir, os.path.basename(img_path).replace('.jpg', '.xml'))\n","        mask = xml_to_mask(xml_path, h, w)\n","        if self.transform:\n","            augmented = self.transform(image=img, mask=mask)\n","            img, mask = augmented['image'], augmented['mask']\n","        img = torch.from_numpy(img.transpose(2,0,1)).float()/255.0\n","        mask = torch.from_numpy(mask).unsqueeze(0).float()\n","        return img, mask\n","\n","transform = A.Compose([\n","    A.Resize(480, 480),\n","    A.HorizontalFlip(p=0.5),\n","    A.RandomBrightnessContrast(p=0.2),\n","])\n","\n","torch.cuda.empty_cache()\n","train_ds = DroneDataset(\n","    img_dir='/content/drone_data/Drone_TrainSet',\n","    xml_dir='/content/drone_data/Drone_TrainSet_XMLs',\n","    transform=transform\n",")\n","train_dl = DataLoader(train_ds, batch_size=4, shuffle=True, num_workers=0)\n","print(f\"Train örnek sayısı: {len(train_ds)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rM-Stu8BcwJ_","executionInfo":{"status":"ok","timestamp":1753438624044,"user_tz":-180,"elapsed":116,"user":{"displayName":"Türker Efe Soyutemiz","userId":"11416081914510237351"}},"outputId":"7848f442-ec71-49c1-9b44-858e81ad81ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train örnek sayısı: 51446\n"]}]},{"cell_type":"code","source":["# Hücre 4: U‑Net Modeli\n","import segmentation_models_pytorch as smp\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","unet_model = smp.Unet(\n","    encoder_name='resnet34', encoder_weights='imagenet', in_channels=3, classes=1\n",").to(device)\n","\n","unet_weights = '/content/drive/MyDrive/unet_drone_best.pth'\n","if os.path.isfile(unet_weights):\n","    unet_model.load_state_dict(torch.load(unet_weights, map_location=device))\n","unet_model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["5ecba41fcd8c41398b1b0fc67fcd407a","fbbd54b6959b4d878dc7e7798326039a","ec5791a950164070a6a9dd10f863b758","e65dbba63c9e4cf09463c7d158512cf9","03f82b7948ef43bfae6f1372304a9d60","30243376f61c41369cf7c8ca5e4d3648","de7e297e85e341e4a8d7f07ff9b15697","37fb15ce6e004da896fa54672f847f1e","13820d6c488f4adea1dd784e88a9edfb","30bb00a55dfe41e68952bc9093c0cb1b","a572025fd2a7458fb317e11f1b699ac6","31690c0f0ad64f26bd9e2b8c6d87eb7a","5ff317d428854c258c6d1a3e2c49c4d9","455e13b2954644d5a3dad95f02b7cb26","6e23a4373285440c9fec3cd9e6ef0154","a7d6f438df8b479399c5e9a937ae8676","4f0efaa016a543ea8052a1fc2d27e01c","ae710a7735724987b68268cb9e30908b","ffa42bce56714eaa96c7ba19f1622bb2","3ee7e887479d46de920763d1b3ed09a4","8f9a1487c2804ef1b3aba86785502a7d","8d4ded50acee40f8b769199a69699fa9"]},"id":"t9hxuXMlcxac","executionInfo":{"status":"ok","timestamp":1753411485464,"user_tz":-180,"elapsed":16658,"user":{"displayName":"Türker Efe Soyutemiz","userId":"11416081914510237351"}},"outputId":"51eca09f-7489-478a-b99a-349965ed0d91"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/87.3M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["Unet(\n","  (encoder): ResNetEncoder(\n","    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU(inplace=True)\n","    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (layer1): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (2): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (2): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (3): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (2): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (3): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (4): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (5): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (2): BasicBlock(\n","        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","  )\n","  (decoder): UnetDecoder(\n","    (center): Identity()\n","    (blocks): ModuleList(\n","      (0): UnetDecoderBlock(\n","        (conv1): Conv2dReLU(\n","          (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention1): Attention(\n","          (attention): Identity()\n","        )\n","        (conv2): Conv2dReLU(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention2): Attention(\n","          (attention): Identity()\n","        )\n","      )\n","      (1): UnetDecoderBlock(\n","        (conv1): Conv2dReLU(\n","          (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention1): Attention(\n","          (attention): Identity()\n","        )\n","        (conv2): Conv2dReLU(\n","          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention2): Attention(\n","          (attention): Identity()\n","        )\n","      )\n","      (2): UnetDecoderBlock(\n","        (conv1): Conv2dReLU(\n","          (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention1): Attention(\n","          (attention): Identity()\n","        )\n","        (conv2): Conv2dReLU(\n","          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention2): Attention(\n","          (attention): Identity()\n","        )\n","      )\n","      (3): UnetDecoderBlock(\n","        (conv1): Conv2dReLU(\n","          (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention1): Attention(\n","          (attention): Identity()\n","        )\n","        (conv2): Conv2dReLU(\n","          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention2): Attention(\n","          (attention): Identity()\n","        )\n","      )\n","      (4): UnetDecoderBlock(\n","        (conv1): Conv2dReLU(\n","          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention1): Attention(\n","          (attention): Identity()\n","        )\n","        (conv2): Conv2dReLU(\n","          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention2): Attention(\n","          (attention): Identity()\n","        )\n","      )\n","    )\n","  )\n","  (segmentation_head): SegmentationHead(\n","    (0): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): Identity()\n","    (2): Activation(\n","      (activation): Identity()\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# Hücre 5: YOLOv8 Detection Modeli\n","from ultralytics import YOLO\n","yolo_weights = '/content/drive/MyDrive/drone_model/best.pt'\n","yolo_model = YOLO(yolo_weights)"],"metadata":{"id":"UgHQEOcLcyzi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hücre 7: YOLO Modeli ve Feature Kanal Sayıları\n","from ultralytics import YOLO\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import timm\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","yolo_weights = '/content/drive/MyDrive/drone_model/best.pt'\n","yolo_model = YOLO(yolo_weights).to(device)\n","\n","# İlk feature bloklarını backbone olarak ayırma\n","yolo_backbone = nn.Sequential(*list(yolo_model.model.model.children())[:6]).to(device)\n","\n","# Özellik çıkarım fonksiyonu\n","def get_feature_channels(model, size=(1,3,480,480)):\n","    dummy = torch.zeros(size).to(device)\n","    with torch.no_grad():\n","        out = model(dummy)\n","    feat = out[-1] if isinstance(out, (list, tuple)) else out\n","    return feat.shape[1]\n","\n","# U‑Net encoder (önceden eğitilmiş ağırlıklar)\n","import segmentation_models_pytorch as smp\n","unet_model = smp.Unet(encoder_name='resnet34', encoder_weights=None, in_channels=3, classes=1).to(device)\n","unet_model.load_state_dict(torch.load('/content/drive/MyDrive/unet_drone_best.pth', map_location=device))\n","\n","# YOLO ve U‑Net encoder kanal sayıları\n","cy = get_feature_channels(yolo_backbone)\n","cu = get_feature_channels(unet_model.encoder)\n","print(f\"YOLO channels: {cy}, U‑Net encoder channels: {cu}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WHC6oHwpc0CM","executionInfo":{"status":"ok","timestamp":1753411516365,"user_tz":-180,"elapsed":4022,"user":{"displayName":"Türker Efe Soyutemiz","userId":"11416081914510237351"}},"outputId":"f7fed09c-3684-4ea7-895c-759d9b927897"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["YOLO channels: 640, U‑Net encoder channels: 512\n"]}]},{"cell_type":"code","source":["# Hücre 8\n","import timm\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class HybridModelWithViT(nn.Module):\n","    def __init__(self, yolo_backbone, unet_encoder, cy, cu):\n","        super().__init__()\n","        self.yolo_backbone = yolo_backbone\n","        self.unet_encoder = unet_encoder\n","        self.fuse = nn.Conv2d(cy + cu, 512, kernel_size=1)\n","\n","        # ViT bloğu\n","        self.vit_block = timm.create_model(\n","            'vit_tiny_patch16_224',\n","            pretrained=True,\n","            in_chans=512,\n","            num_classes=0\n","        )\n","\n","        # Decoder ve Head katmanları\n","        self.decoder = nn.Conv2d(192, 512, kernel_size=1)\n","        self.head = nn.Conv2d(512, 1, kernel_size=1)\n","\n","    def forward(self, x):\n","        # Backbone'lardan feature map çıkarımı\n","        f_y = self.yolo_backbone(x)\n","        f_u = self.unet_encoder(x)[-1]\n","\n","        # Spatial boyutları eşitleme\n","        if f_u.shape[2:] != f_y.shape[2:]:\n","            f_u = F.interpolate(f_u, size=f_y.shape[2:], mode='bilinear', align_corners=False)\n","\n","        # Kanal boyutunda birleştirme ve fuse etme\n","        m = torch.cat([f_y, f_u], dim=1)\n","        fused_features = self.fuse(m)\n","\n","        # ViT'ye göndermeden önce öznitelik haritasını 224x224'e büyüt.\n","        vit_input = F.interpolate(fused_features, size=(224, 224), mode='bilinear', align_corners=False)\n","        vit_output = self.vit_block.forward_features(vit_input)\n","\n","        # ViT çıktısını tekrar 2D formata dönüştürme.\n","        B, _, C = vit_output.shape\n","        H_vit, W_vit = 14, 14\n","        vit_features_reshaped = vit_output[:, 1:, :].permute(0, 2, 1).reshape(B, C, H_vit, W_vit)\n","\n","        # ViT'den gelen öznitelik haritasını, orijinal küçük boyuta geri küçültme.\n","        original_H, original_W = fused_features.shape[2:]\n","        vit_features_downscaled = F.interpolate(vit_features_reshaped, size=(original_H, original_W), mode='bilinear', align_corners=False)\n","\n","        # ViT çıktısını işleyip orijinal öznitelik haritasına ekleme\n","        decoded_vit_features = self.decoder(vit_features_downscaled)\n","        final_features = fused_features + decoded_vit_features\n","        return self.head(final_features)\n","\n","hybrid_model = HybridModelWithViT(yolo_backbone, unet_model.encoder, cy, cu).to(device)\n","print(hybrid_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9FLjvUJSc1NN","executionInfo":{"status":"ok","timestamp":1753411883947,"user_tz":-180,"elapsed":791,"user":{"displayName":"Türker Efe Soyutemiz","userId":"11416081914510237351"}},"outputId":"db46872b-eb71-427f-ca93-1308f0b48532"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- ValueError Düzeltilmiş Hibrit Model Mimarisi ---\n","HybridModelWithViT(\n","  (yolo_backbone): Sequential(\n","    (0): Conv(\n","      (conv): Conv2d(3, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","      (act): SiLU(inplace=True)\n","    )\n","    (1): Conv(\n","      (conv): Conv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","      (act): SiLU(inplace=True)\n","    )\n","    (2): C2f(\n","      (cv1): Conv(\n","        (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","        (act): SiLU(inplace=True)\n","      )\n","      (cv2): Conv(\n","        (conv): Conv2d(400, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","        (act): SiLU(inplace=True)\n","      )\n","      (m): ModuleList(\n","        (0-2): 3 x Bottleneck(\n","          (cv1): Conv(\n","            (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","            (act): SiLU(inplace=True)\n","          )\n","        )\n","      )\n","    )\n","    (3): Conv(\n","      (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","      (act): SiLU(inplace=True)\n","    )\n","    (4): C2f(\n","      (cv1): Conv(\n","        (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","        (act): SiLU(inplace=True)\n","      )\n","      (cv2): Conv(\n","        (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","        (act): SiLU(inplace=True)\n","      )\n","      (m): ModuleList(\n","        (0-5): 6 x Bottleneck(\n","          (cv1): Conv(\n","            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","            (act): SiLU(inplace=True)\n","          )\n","        )\n","      )\n","    )\n","    (5): Conv(\n","      (conv): Conv2d(320, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","      (act): SiLU(inplace=True)\n","    )\n","  )\n","  (unet_encoder): ResNetEncoder(\n","    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU(inplace=True)\n","    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (layer1): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (2): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (2): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (3): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (2): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (3): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (4): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (5): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (2): BasicBlock(\n","        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","  )\n","  (fuse): Conv2d(1152, 512, kernel_size=(1, 1), stride=(1, 1))\n","  (vit_block): VisionTransformer(\n","    (patch_embed): PatchEmbed(\n","      (proj): Conv2d(512, 192, kernel_size=(16, 16), stride=(16, 16))\n","      (norm): Identity()\n","    )\n","    (pos_drop): Dropout(p=0.0, inplace=False)\n","    (patch_drop): Identity()\n","    (norm_pre): Identity()\n","    (blocks): Sequential(\n","      (0): Block(\n","        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=192, out_features=576, bias=True)\n","          (q_norm): Identity()\n","          (k_norm): Identity()\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (proj): Linear(in_features=192, out_features=192, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls1): Identity()\n","        (drop_path1): Identity()\n","        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=192, out_features=768, bias=True)\n","          (act): GELU(approximate='none')\n","          (drop1): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (fc2): Linear(in_features=768, out_features=192, bias=True)\n","          (drop2): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls2): Identity()\n","        (drop_path2): Identity()\n","      )\n","      (1): Block(\n","        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=192, out_features=576, bias=True)\n","          (q_norm): Identity()\n","          (k_norm): Identity()\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (proj): Linear(in_features=192, out_features=192, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls1): Identity()\n","        (drop_path1): Identity()\n","        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=192, out_features=768, bias=True)\n","          (act): GELU(approximate='none')\n","          (drop1): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (fc2): Linear(in_features=768, out_features=192, bias=True)\n","          (drop2): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls2): Identity()\n","        (drop_path2): Identity()\n","      )\n","      (2): Block(\n","        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=192, out_features=576, bias=True)\n","          (q_norm): Identity()\n","          (k_norm): Identity()\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (proj): Linear(in_features=192, out_features=192, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls1): Identity()\n","        (drop_path1): Identity()\n","        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=192, out_features=768, bias=True)\n","          (act): GELU(approximate='none')\n","          (drop1): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (fc2): Linear(in_features=768, out_features=192, bias=True)\n","          (drop2): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls2): Identity()\n","        (drop_path2): Identity()\n","      )\n","      (3): Block(\n","        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=192, out_features=576, bias=True)\n","          (q_norm): Identity()\n","          (k_norm): Identity()\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (proj): Linear(in_features=192, out_features=192, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls1): Identity()\n","        (drop_path1): Identity()\n","        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=192, out_features=768, bias=True)\n","          (act): GELU(approximate='none')\n","          (drop1): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (fc2): Linear(in_features=768, out_features=192, bias=True)\n","          (drop2): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls2): Identity()\n","        (drop_path2): Identity()\n","      )\n","      (4): Block(\n","        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=192, out_features=576, bias=True)\n","          (q_norm): Identity()\n","          (k_norm): Identity()\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (proj): Linear(in_features=192, out_features=192, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls1): Identity()\n","        (drop_path1): Identity()\n","        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=192, out_features=768, bias=True)\n","          (act): GELU(approximate='none')\n","          (drop1): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (fc2): Linear(in_features=768, out_features=192, bias=True)\n","          (drop2): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls2): Identity()\n","        (drop_path2): Identity()\n","      )\n","      (5): Block(\n","        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=192, out_features=576, bias=True)\n","          (q_norm): Identity()\n","          (k_norm): Identity()\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (proj): Linear(in_features=192, out_features=192, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls1): Identity()\n","        (drop_path1): Identity()\n","        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=192, out_features=768, bias=True)\n","          (act): GELU(approximate='none')\n","          (drop1): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (fc2): Linear(in_features=768, out_features=192, bias=True)\n","          (drop2): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls2): Identity()\n","        (drop_path2): Identity()\n","      )\n","      (6): Block(\n","        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=192, out_features=576, bias=True)\n","          (q_norm): Identity()\n","          (k_norm): Identity()\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (proj): Linear(in_features=192, out_features=192, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls1): Identity()\n","        (drop_path1): Identity()\n","        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=192, out_features=768, bias=True)\n","          (act): GELU(approximate='none')\n","          (drop1): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (fc2): Linear(in_features=768, out_features=192, bias=True)\n","          (drop2): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls2): Identity()\n","        (drop_path2): Identity()\n","      )\n","      (7): Block(\n","        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=192, out_features=576, bias=True)\n","          (q_norm): Identity()\n","          (k_norm): Identity()\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (proj): Linear(in_features=192, out_features=192, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls1): Identity()\n","        (drop_path1): Identity()\n","        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=192, out_features=768, bias=True)\n","          (act): GELU(approximate='none')\n","          (drop1): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (fc2): Linear(in_features=768, out_features=192, bias=True)\n","          (drop2): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls2): Identity()\n","        (drop_path2): Identity()\n","      )\n","      (8): Block(\n","        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=192, out_features=576, bias=True)\n","          (q_norm): Identity()\n","          (k_norm): Identity()\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (proj): Linear(in_features=192, out_features=192, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls1): Identity()\n","        (drop_path1): Identity()\n","        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=192, out_features=768, bias=True)\n","          (act): GELU(approximate='none')\n","          (drop1): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (fc2): Linear(in_features=768, out_features=192, bias=True)\n","          (drop2): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls2): Identity()\n","        (drop_path2): Identity()\n","      )\n","      (9): Block(\n","        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=192, out_features=576, bias=True)\n","          (q_norm): Identity()\n","          (k_norm): Identity()\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (proj): Linear(in_features=192, out_features=192, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls1): Identity()\n","        (drop_path1): Identity()\n","        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=192, out_features=768, bias=True)\n","          (act): GELU(approximate='none')\n","          (drop1): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (fc2): Linear(in_features=768, out_features=192, bias=True)\n","          (drop2): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls2): Identity()\n","        (drop_path2): Identity()\n","      )\n","      (10): Block(\n","        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=192, out_features=576, bias=True)\n","          (q_norm): Identity()\n","          (k_norm): Identity()\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (proj): Linear(in_features=192, out_features=192, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls1): Identity()\n","        (drop_path1): Identity()\n","        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=192, out_features=768, bias=True)\n","          (act): GELU(approximate='none')\n","          (drop1): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (fc2): Linear(in_features=768, out_features=192, bias=True)\n","          (drop2): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls2): Identity()\n","        (drop_path2): Identity()\n","      )\n","      (11): Block(\n","        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=192, out_features=576, bias=True)\n","          (q_norm): Identity()\n","          (k_norm): Identity()\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (proj): Linear(in_features=192, out_features=192, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls1): Identity()\n","        (drop_path1): Identity()\n","        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=192, out_features=768, bias=True)\n","          (act): GELU(approximate='none')\n","          (drop1): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (fc2): Linear(in_features=768, out_features=192, bias=True)\n","          (drop2): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls2): Identity()\n","        (drop_path2): Identity()\n","      )\n","    )\n","    (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n","    (fc_norm): Identity()\n","    (head_drop): Dropout(p=0.0, inplace=False)\n","    (head): Identity()\n","  )\n","  (decoder): Conv2d(192, 512, kernel_size=(1, 1), stride=(1, 1))\n","  (head): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",")\n"]}]},{"cell_type":"code","source":["# Hücre 9\n","import os\n","import cv2\n","import numpy as np\n","import xml.etree.ElementTree as ET\n","from torch.utils.data import Dataset\n","\n","class DroneSegDataset(Dataset):\n","    def __init__(self, img_dir, xml_dir, transform=None):\n","        self.img_paths = sorted([os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.lower().endswith('.jpg')])\n","        self.xml_dir = xml_dir\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.img_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.img_paths[idx]\n","        img = cv2.imread(img_path)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        h, w = img.shape[:2]\n","\n","        # XML to mask\n","        xml_path = os.path.join(self.xml_dir, os.path.basename(img_path).replace('.jpg', '.xml'))\n","        mask = np.zeros((h, w), dtype=np.uint8)\n","        if os.path.isfile(xml_path):\n","            tree = ET.parse(xml_path)\n","            for obj in tree.findall('object'):\n","                bbox = obj.find('bndbox')\n","                xmin = int(float(bbox.find('xmin').text))\n","                ymin = int(float(bbox.find('ymin').text))\n","                xmax = int(float(bbox.find('xmax').text))\n","                ymax = int(float(bbox.find('ymax').text))\n","                mask[ymin:ymax, xmin:xmax] = 1\n","\n","        if self.transform:\n","            augmented = self.transform(image=img, mask=mask)\n","            img, mask = augmented['image'], augmented['mask']\n","\n","        return img, mask"],"metadata":{"id":"wvF4hj_7c6GG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hücre 10\n","import torch.nn.functional as F\n","from tqdm.auto import tqdm\n","\n","\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from torch.utils.data import DataLoader\n","\n","train_img_dir = '/content/drone_data/Drone_TrainSet'\n","train_xml_dir = '/content/drone_data/Drone_TrainSet_XMLs'\n","batch_size, epochs = 8, 10\n","\n","\n","transform = A.Compose([\n","    A.Resize(480, 640),\n","    A.HorizontalFlip(0.5),\n","    A.Normalize(),\n","    ToTensorV2()\n","])\n","\n","# Dataset ve DataLoader\n","train_ds = DroneSegDataset(train_img_dir, train_xml_dir, transform=transform)\n","train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n","\n","# Optimizör, artık ViT içeren `hybrid_model`'in parametrelerini alacak.\n","optimizer = torch.optim.Adam(hybrid_model.parameters(), lr=1e-4)\n","criterion = nn.BCEWithLogitsLoss()\n","\n","loss_history = []\n","hybrid_model.train()\n","for e in range(1, epochs+1):\n","    running = 0.0\n","    loop = tqdm(train_loader, desc=f\"Epoch {e}/{epochs}\", leave=False)\n","    for imgs, masks in loop:\n","        imgs = imgs.to(device)\n","        # mask'e bir kanal boyutu ekliyoruz ve float'a çeviriyoruz.\n","        masks = masks.to(device).float().unsqueeze(1)\n","        optimizer.zero_grad()\n","        preds = hybrid_model(imgs)\n","\n","        # Eğer spatial boyutlar eşleşmiyorsa mask'e upsample\n","        if preds.shape[2:] != masks.shape[2:]:\n","            preds = F.interpolate(preds, size=masks.shape[2:], mode='bilinear', align_corners=False)\n","\n","        loss = criterion(preds, masks)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running += loss.item()\n","        loop.set_postfix(loss=running / (loop.n + 1))\n","\n","    avg = running / len(train_loader)\n","    loss_history.append(avg)\n","    print(f\"Epoch {e}/{epochs} - Loss: {avg:.4f}\")\n","print(\"Eğitim tamamlandı!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":208,"referenced_widgets":["45872db01ad14d9284757a3294810850","92bdadd9482947beb45b1b5191b62977","b1303337f7f34700945bfdca6b767eb4","36257d5655444ad5b5a18e3b811935e7","4bdde78918e14045abd4ff477430c8c9","737bd866bf41475c96935e92149e0679","4cf18c74976647588f5f2d707f10a84d","b119813737a94449878256572a6b6f13","4fa87d56123d4466bdc7b739796022ea","634c630799a848b1982149f057781f3e","c4d601acc0f243a4b6d513cfd5334f3a","bb429ef133c74dc1bd0aff9fb718f749","d542fdb78ce94d19a7fb1b9c419fde93","a19cbe96c1b349efb4ba27a35004bfe0","f168a7aad1644f99a5037c7a48bfd5d6","cc6489c21f4840638f03c1e9a8695c62","57eff1f18377435d99a3facab2c975d8","d60a9122534c4c49a47e80d7e8ee2862","26f1c494e8334793835a6aca411824f5","31c5c66284444c48ab3b30d5595c3f36","57e8554f94b3412e82c9d21eab9d4dc0","ecb1a1dec702402181a200067e2967c9","666455586cb14ffcbcc772b6b20a00be","5d3769f51b1b44bfadaaec05c28048a8","50a5bba5040c43f685589681e46f7629","bf3be0fad68f40b18109d8fea7dd99d2","14b568c015b142c5b4add618d748b72f","70f1d306ea9e4239934c7f3037a08953","54d0468d9ac94a3bb99d78b89aef61d1","928043b8c06b4949bff6f22d8115e1d7","093aac06b70c4946b6fdb8b9e5cdb693","da83a0be9ba84f29af482d49b1eecf79","250ea26c0b394f4da4720b9833d0dc3f","6d98300253c54fee968d9e63920aaf0d","1dacc95c1db747d19e265295751c1366","78dfd91f5cb642e49672acf2f902f1eb","9455b749e9a04a809eec7b3bb8d98ab0","b49f807da5d543679148f0c7243335d0","45278aa9aac64696b5355225f3e725f6","f18b7095312943b08190c7cec6c499c8","53518b785d8d4fc0bccf1ab4166867e7","609cef78ba4a41b3a9373f52b6d4a516","b9c8620941b34389a014cd5e6e34131a","11faced2a8974826b9e67c6f4aa89e06","0fb2f3765b1b422cbb0555fc92c514b4","2de1c4584c7742eb894b542d2540742b","8af9879dca6d4c8abe1ec1bf5d78c810","389b41ce115647a99e521a05445fec40","a9c66a0cad364681a7be97b4e28d2a4c","a52becaa76494cc7af766c0cc5a8a306","172e178848324c3984893ea9c0dcd713","2c7fe25fc55f4b8e96f1668191c4d702","816c69e9b54b4fd6bd3e89c2b765f0f4","8fc0d839d6574b2f8ef9b7a9643b3daf","78607d8c968146dd9b800b257083e243","0bcb1422081249dba6f9d99822e84b0c","c27ad3dc07dc470aaa47caac23f38ce1","5f917dc9688941d595824524e17ff9a0","0f9a975251cd411a8f06b5d4e0a7a444","ae2e7dbe20044dd382bd8e1209ee11e0","b945cc8769084c2c81a8b1a976956f05","844c40c4d21642eb9b278d88cc9b95fb","80f943f00d8e4618b37b04a3b769b547","8709cacb4a074b4ca01ded158d460379","f227be38012342b281aea94c87f2160a","18eaeb04ae8a42f9ba6dd2d9049e37e3","c08a8413f48741189ec7a0874b778b34","5d9341af75a64cab80e6eb0df16ac2a5","6124ba7b14164f4a9b584fd07887e133","c5504389a6c747288bcb60e2ca952e1b","e7886cdd971240b3bc7c0a636bd0a362","2b61c4e551794d8d9da8464e00fea547","71d3a92cb3ef4c6b8e791e62b99966c1","b884449fc4b64a318c7024dceae87ca3","19269b85f18f477fa361a3249fd6ad92","db49120bb90b4f898beef48c6fe62535","b7338739913f44e4b183255c7e2a61c7","e164133a527a4a639d55bfa16365ff6f","246c4c83d1fa43e8b4d99b0a04271ec5","e1dee5e02d1e40a8b17ffcb737d4f51c","153fb6b628fe484b8deea86f49ef6ce5","2e19b024434b42038d82c619653866c1","19c537e4f7014b0097b7c0e759b431e4","976d4678e23b40cb81b74dfff9b786c1","022f164926cf4de5849beef6c6f3d860","b8d64acaab6b4993b6fc85a35d365c65","4fcbec70c16e473f8be5c8cd132503ff","492a20c7e9b743a0ad6004fc10cbd8d4","9579533c2dbd4c59a035a25a8bf824a2","00711f4781eb4c488566f5f5e6cbb217","985c8e8ee3614e53b22417de192b2a82","547cc76ca2394dac95a6aaf3143fcdcd","560400d19306407ba990c4957452e7a4","bea9e05ed716417d8f13156b83f9a3ca","e2d2c31813634a8399d45218f7e77fff","c97d9ccb6fdf43019d4400ad4f5ff291","053eabd46fcb4ac882505142d8e8021b","e1f84b861ec74664a66fd832e841db53","a8341c653c0b4bf5847d95aa832ff241","3fc6a2718c3b4257be4ce54921c5484b","1025930df70d43f5baff69921a249b91","80f083b8dc44411da12bd34cf16b6ab8","38d6f2a4bb224403b17bc299e1c50327","df5292bfcfe44077aa9aea7d5d3fdb5d","82dbe343f8394fc5bd9ed27553d7c52d","780ac8cf09314e76ae2c6998370d639b","26f105ce01334e6da9c2d13fbb525193","8ddcf600a2554b3c8a3f4d6d16588980","627f723e75004c2cb6b9e0d36ae464e9","3837b24f67d1445d8ba10a3bfa1d32c8"]},"id":"uuqla9QAdCUB","executionInfo":{"status":"ok","timestamp":1753423131354,"user_tz":-180,"elapsed":10130783,"user":{"displayName":"Türker Efe Soyutemiz","userId":"11416081914510237351"}},"outputId":"7a352d11-05f4-40bb-ef6a-7617e440eaf4"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Epoch 1/10:   0%|          | 0/6431 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 1/10 - Loss: 0.0182\n"]},{"output_type":"display_data","data":{"text/plain":["Epoch 2/10:   0%|          | 0/6431 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 2/10 - Loss: 0.0157\n"]},{"output_type":"display_data","data":{"text/plain":["Epoch 3/10:   0%|          | 0/6431 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 3/10 - Loss: 0.0138\n"]},{"output_type":"display_data","data":{"text/plain":["Epoch 4/10:   0%|          | 0/6431 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 4/10 - Loss: 0.0131\n"]},{"output_type":"display_data","data":{"text/plain":["Epoch 5/10:   0%|          | 0/6431 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 5/10 - Loss: 0.0121\n"]},{"output_type":"display_data","data":{"text/plain":["Epoch 6/10:   0%|          | 0/6431 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 6/10 - Loss: 0.0116\n"]},{"output_type":"display_data","data":{"text/plain":["Epoch 7/10:   0%|          | 0/6431 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 7/10 - Loss: 0.0109\n"]},{"output_type":"display_data","data":{"text/plain":["Epoch 8/10:   0%|          | 0/6431 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 8/10 - Loss: 0.0106\n"]},{"output_type":"display_data","data":{"text/plain":["Epoch 9/10:   0%|          | 0/6431 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 9/10 - Loss: 0.0105\n"]},{"output_type":"display_data","data":{"text/plain":["Epoch 10/10:   0%|          | 0/6431 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 10/10 - Loss: 0.0098\n","Eğitim tamamlandı!\n"]}]},{"cell_type":"code","source":["full_model_save_path = '/content/drive/MyDrive/hybrid_drone_model_with_vit_FULL.pth'\n","\n","# Modeli belirtilen yola kaydet\n","torch.save(hybrid_model, full_model_save_path)\n","\n","print(f\"Model kaydedildi!\")\n","print(f\"Yol: {full_model_save_path}\")\n","print(\"\\n\" + \"=\"*50)\n","print(\"=\"*50 + \"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cuXM8Ju2jxXu","executionInfo":{"status":"ok","timestamp":1753423131970,"user_tz":-180,"elapsed":600,"user":{"displayName":"Türker Efe Soyutemiz","userId":"11416081914510237351"}},"outputId":"2b239392-0c3a-481d-e7c3-86b938e4208f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model kaydedildi!\n","Yol: /content/drive/MyDrive/hybrid_drone_model_with_vit_FULL.pth\n","\n","==================================================\n","==================================================\n","\n"]}]},{"cell_type":"code","source":["import gdown\n","import shutil\n","import os\n","from tqdm.auto import tqdm\n","import matplotlib.pyplot as plt\n","import torch\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from torch.utils.data import DataLoader\n","import numpy as np\n","\n","\n","validation_file_id = '1dJdUIqTfW7InN76Xb3flWVrVmcficP93'\n","dest_folder = '/content/VALIDATION'\n","zip_path = f\"{dest_folder}.zip\"\n","\n","print(\"\\n[ADIM 1/4] Doğrulama ZIP dosyası indiriliyor...\")\n","\n","if os.path.exists(dest_folder):\n","    shutil.rmtree(dest_folder)\n","os.makedirs(dest_folder, exist_ok=True)\n","gdown.download(id=validation_file_id, output=zip_path, quiet=False)\n","print(f\"'{os.path.basename(dest_folder)}' klasörüne dosyalar açılıyor...\")\n","!unzip -q -o {zip_path} -d {dest_folder}\n","\n","\n","val_img_dir = os.path.join(dest_folder, 'testset')\n","val_xml_dir = os.path.join(dest_folder, 'test.xml')\n","\n","if not os.path.isdir(val_img_dir) or not os.path.isdir(val_xml_dir):\n","    print(\"\\n!!! HATA !!!\")\n","    print(f\"ZIP açıldıktan sonra '{val_img_dir}' veya '{val_xml_dir}' klasörleri bulunamadı.\")\n","    print(\"Lütfen ZIP dosyasının içeriğini ve klasör isimlerini kontrol edin.\")\n","    print(\"Mevcut içerik:\", os.listdir(dest_folder))\n","else:\n","    print(\"\\n[ADIM 2/4] Doğrulama verileri için DataLoader oluşturuluyor...\")\n","\n","\n","    val_transform = A.Compose([\n","        A.Resize(480, 640),\n","        A.Normalize(),\n","        ToTensorV2()\n","    ])\n","\n","    validation_dataset = DroneSegDataset(val_img_dir, val_xml_dir, transform=val_transform)\n","    validation_loader = DataLoader(validation_dataset, batch_size=8, shuffle=False, num_workers=2)\n","    print(f\"Yeni doğrulama seti oluşturuldu: {len(validation_dataset)} örnek.\")\n","\n","    print(\"\\n[ADIM 3/4] En iyi model yükleniyor ve metrikler hesaplanıyor...\")\n","    best_model_path = '/content/drive/MyDrive/hybrid_model_best.pth' # Eğitimde kaydedilen yol\n","\n","    if not os.path.exists(best_model_path):\n","        print(f\"Hata: En iyi model bulunamadı: {best_model_path}\")\n","    else:\n","        # Modeli yükle ve değerlendirme moduna al\n","        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","        best_model = torch.load(best_model_path).to(device)\n","        best_model.eval()\n","\n","        # Metrikleri hesapla\n","        total_iou, total_dice, total_pixel_acc, num_batches = 0, 0, 0, len(validation_loader)\n","        eval_loop = tqdm(validation_loader, desc=\"Metrikler Hesaplanıyor\")\n","\n","        with torch.no_grad():\n","            for imgs, masks in eval_loop:\n","                imgs = imgs.to(device)\n","                masks = masks.to(device).float().unsqueeze(1)\n","                preds = torch.sigmoid(best_model(imgs))\n","                preds_binary = (preds > 0.5).float()\n","\n","                intersection = torch.sum(preds_binary * masks)\n","                union = torch.sum(preds_binary) + torch.sum(masks) - intersection\n","                total_iou += ((intersection + 1e-6) / (union + 1e-6)).item()\n","                total_dice += ((2. * intersection + 1e-6) / (torch.sum(preds_binary) + torch.sum(masks) + 1e-6)).item()\n","                total_pixel_acc += (preds_binary == masks).sum().item() / masks.numel()\n","\n","        print(\"\\n--- SONUÇLAR ---\")\n","        print(f\"Ortalama Piksel Doğruluğu: {total_pixel_acc / num_batches:.4f}\")\n","        print(f\"Ortalama IoU (Jaccard Index): {total_iou / num_batches:.4f}\")\n","        print(f\"Ortalama Dice Katsayısı: {total_dice / num_batches:.4f}\")\n","        print(\"--------------------------------------\\n\")\n","\n","        # Görsel Değerlendirme\n","        print(\"[ADIM 4/4] Görsel sonuçlar oluşturuluyor...\")\n","\n","        def denormalize(tensor):\n","            mean, std = np.array([0.485, 0.456, 0.406]), np.array([0.229, 0.224, 0.225])\n","            img_np = tensor.cpu().numpy().transpose(1, 2, 0)\n","            return np.clip(std * img_np + mean, 0, 1)\n","\n","        # Bir batch al ve görselleştir\n","        images, ground_truths = next(iter(validation_loader))\n","        images = images.to(device)\n","        with torch.no_grad():\n","            predictions = torch.sigmoid(best_model(images)) > 0.5\n","\n","        plt.figure(figsize=(15, len(images) * 5))\n","        for i in range(len(images)):\n","            plt.subplot(len(images), 3, i * 3 + 1)\n","            plt.imshow(denormalize(images[i]))\n","            plt.title(f\"Giriş Görüntüsü #{i+1}\")\n","            plt.axis('off')\n","\n","            plt.subplot(len(images), 3, i * 3 + 2)\n","            plt.imshow(ground_truths[i].squeeze(), cmap='gray')\n","            plt.title(f\"Gerçek Maske #{i+1}\")\n","            plt.axis('off')\n","\n","            plt.subplot(len(images), 3, i * 3 + 3)\n","            plt.imshow(predictions[i].squeeze().cpu().numpy(), cmap='gray')\n","            plt.title(f\"Tahmin Edilen Maske #{i+1}\")\n","            plt.axis('off')\n","\n","        plt.tight_layout()\n","        plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q_-gxg_Cg70g","executionInfo":{"status":"ok","timestamp":1753423153556,"user_tz":-180,"elapsed":21568,"user":{"displayName":"Türker Efe Soyutemiz","userId":"11416081914510237351"}},"outputId":"2fbcc76b-4aed-465c-f167-c996626b9d80"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","[ADIM 1/4] Doğrulama ZIP dosyası indiriliyor...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading...\n","From (original): https://drive.google.com/uc?id=1dJdUIqTfW7InN76Xb3flWVrVmcficP93\n","From (redirected): https://drive.google.com/uc?id=1dJdUIqTfW7InN76Xb3flWVrVmcficP93&confirm=t&uuid=f91e2d34-79aa-460a-9ce5-648961eb932a\n","To: /content/VALIDATION.zip\n","100%|██████████| 569M/569M [00:14<00:00, 40.4MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["'VALIDATION' klasörüne dosyalar açılıyor...\n","\n","!!! HATA !!!\n","ZIP açıldıktan sonra '/content/VALIDATION/testset' veya '/content/VALIDATION/test.xml' klasörleri bulunamadı.\n","Lütfen ZIP dosyasının içeriğini ve klasör isimlerini kontrol edin.\n","Mevcut içerik: ['Drone_TestSet_XMLs', 'Drone_TestSet']\n"]}]},{"cell_type":"code","source":["# HÜCRE 12: Eğitim Sonrası Değerlendirme (BU HÜCRE GÜNCELLENDİ)\n","# ==============================================================================\n","import matplotlib.pyplot as plt\n","\n","# Loss grafiği\n","plt.figure(figsize=(8,5))\n","plt.plot(range(1, epochs+1), loss_history, marker='o')\n","plt.title('Training Loss (Hybrid Model with ViT)') # <<< ViT ENTEGRASYONU >>> Başlık güncellendi\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.grid(True)\n","plt.show()\n","\n","# Pixel doğruluğu hesabı\n","# Değerlendirme kodu, herhangi bir ikili segmentasyon modeli için çalıştığından değiştirilmesine gerek yoktur.\n","hybrid_model.eval()\n","total_pixels = 0\n","correct_pixels = 0\n","with torch.no_grad():\n","    for imgs, masks in train_loader:\n","        imgs = imgs.to(device)\n","        masks = masks.to(device).float().unsqueeze(1)\n","        preds = hybrid_model(imgs)\n","        preds = torch.sigmoid(preds)\n","        preds = (preds > 0.5).float()\n","        if preds.shape[2:] != masks.shape[2:]:\n","            preds = F.interpolate(preds, size=masks.shape[2:], mode='nearest')\n","        correct_pixels += (preds == masks).sum().item()\n","        total_pixels += masks.numel()\n","accuracy = correct_pixels / total_pixels\n","print(f\"Training Pixel Accuracy (Hybrid with ViT): {accuracy:.4f}\") # <<< ViT ENTEGRASYONU >>> Çıktı metni güncellendi\n","\n","# Validation döngüsü (Eğer `val_loader` tanımlıysa)\n","try:\n","    total_pixels = 0\n","    correct_pixels = 0\n","    for imgs, masks in val_loader:\n","        imgs = imgs.to(device)\n","        masks = masks.to(device).float().unsqueeze(1)\n","        preds = hybrid_model(imgs)\n","        preds = torch.sigmoid(preds)\n","        preds = (preds > 0.5).float()\n","        if preds.shape[2:] != masks.shape[2:]:\n","            preds = F.interpolate(preds, size=masks.shape[2:], mode='nearest')\n","        correct_pixels += (preds == masks).sum().item()\n","        total_pixels += masks.numel()\n","    val_accuracy = correct_pixels / total_pixels\n","    print(f\"Validation Pixel Accuracy (Hybrid with ViT): {val_accuracy:.4f}\") # <<< ViT ENTEGRASYONU >>> Çıktı metni güncellendi\n","except NameError:\n","    pass\n","\n","# <<< ViT ENTEGRASYONU >>> Modelin son ağırlıklarını yeni bir isimle kaydediyoruz.\n","save_path = '/content/drive/MyDrive/hybrid_drone_model_with_vit.pth'\n","torch.save(hybrid_model.state_dict(), save_path)\n","print(f'ViT entegreli model ağırlıkları kaydedildi: {save_path}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":698},"id":"DY6k_lJldJkV","executionInfo":{"status":"error","timestamp":1753425532255,"user_tz":-180,"elapsed":578801,"user":{"displayName":"Türker Efe Soyutemiz","userId":"11416081914510237351"}},"outputId":"0d352180-9c76-498b-9ffc-989e78449a01"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 800x500 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Training Pixel Accuracy (Hybrid with ViT): 0.9945\n"]},{"output_type":"error","ename":"TypeError","evalue":"'NoneType' object is not iterable","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-24-2387890815.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mtotal_pixels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mcorrect_pixels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"]}]},{"cell_type":"code","source":["# ==============================================================================\n","# BÖLÜM 1: YENİ OTURUM İÇİN ORTAM KURULUMU VE GEREKLİ TANIMLAMALAR\n","# ==============================================================================\n","print(\">>> BÖLÜM 1: Ortam kuruluyor ve gerekli sınıflar tanımlanıyor...\")\n","\n","# Gerekli kütüphaneleri kur/güncelle\n","%pip install --upgrade ultralytics segmentation-models-pytorch timm albumentations opencv-python matplotlib gdown tqdm -q\n","\n","# Gerekli tüm kütüphaneleri import et\n","import os\n","import shutil\n","import gdown\n","from tqdm.auto import tqdm\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import cv2\n","import numpy as np\n","import xml.etree.ElementTree as ET\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from google.colab import drive\n","import matplotlib.pyplot as plt\n","import timm\n","from ultralytics import YOLO\n","import segmentation_models_pytorch as smp\n","\n","# Google Drive'ı bağla\n","drive.mount('/content/drive')\n","\n","# --- Veri Seti Sınıfı Tanımı ---\n","class DroneSegDataset(Dataset):\n","    def __init__(self, img_dir, xml_dir, transform=None):\n","        self.img_paths = sorted([os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.lower().endswith('.jpg')])\n","        self.xml_dir = xml_dir\n","        self.transform = transform\n","    def __len__(self): return len(self.img_paths)\n","    def __getitem__(self, idx):\n","        img_path = self.img_paths[idx]\n","        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n","        h, w = img.shape[:2]\n","        mask = np.zeros((h, w), dtype=np.uint8)\n","        xml_path = os.path.join(self.xml_dir, os.path.basename(img_path).replace('.jpg', '.xml'))\n","        if os.path.isfile(xml_path):\n","            tree = ET.parse(xml_path)\n","            for obj in tree.findall('object'):\n","                bbox = obj.find('bndbox')\n","                xmin, ymin = int(float(bbox.find('xmin').text)), int(float(bbox.find('ymin').text))\n","                xmax, ymax = int(float(bbox.find('xmax').text)), int(float(bbox.find('ymax').text))\n","                mask[ymin:ymax, xmin:xmax] = 1\n","        if self.transform:\n","            augmented = self.transform(image=img, mask=mask)\n","            img, mask = augmented['image'], augmented['mask']\n","        return img, mask\n","\n","# --- Model Mimarisi Sınıfı Tanımı ---\n","class HybridModelWithViT(nn.Module):\n","    def __init__(self, yolo_backbone, unet_encoder, cy, cu):\n","        super().__init__()\n","        self.yolo_backbone = yolo_backbone\n","        self.unet_encoder = unet_encoder\n","        self.fuse = nn.Conv2d(cy + cu, 512, kernel_size=1)\n","        self.vit_block = timm.create_model('vit_tiny_patch16_224', pretrained=True, in_chans=512, num_classes=0)\n","        self.decoder = nn.Conv2d(192, 512, kernel_size=1)\n","        self.head = nn.Conv2d(512, 1, kernel_size=1)\n","    def forward(self, x):\n","        f_y = self.yolo_backbone(x)\n","        f_u = self.unet_encoder(x)[-1]\n","        if f_u.shape[2:] != f_y.shape[2:]: f_u = F.interpolate(f_u, size=f_y.shape[2:], mode='bilinear', align_corners=False)\n","        m = torch.cat([f_y, f_u], dim=1)\n","        fused_features = self.fuse(m)\n","        vit_input = F.interpolate(fused_features, size=(224, 224), mode='bilinear', align_corners=False)\n","        vit_output = self.vit_block.forward_features(vit_input)\n","        B, _, C = vit_output.shape\n","        H_vit, W_vit = 14, 14\n","        vit_features_reshaped = vit_output[:, 1:, :].permute(0, 2, 1).reshape(B, C, H_vit, W_vit)\n","        original_H, original_W = fused_features.shape[2:]\n","        vit_features_downscaled = F.interpolate(vit_features_reshaped, size=(original_H, original_W), mode='bilinear', align_corners=False)\n","        decoded_vit_features = self.decoder(vit_features_downscaled)\n","        final_features = fused_features + decoded_vit_features\n","        return self.head(final_features)\n","\n","print(\">>> BÖLÜM 1: Kurulum ve tanımlamalar tamamlandı.\\n\")\n","\n","\n","print(\" BÖLÜM 2: Özel doğrulama süreci \")\n","\n","# 1. Özel Doğrulama Setini İndir ve Hazırla\n","validation_file_id = '1dJdUIqTfW7InN76Xb3flWVrVmcficP93'\n","dest_folder = '/content/VALIDATION'\n","zip_path = f\"{dest_folder}.zip\"\n","\n","if not os.path.exists(dest_folder):\n","    print(\"Doğrulama ZIP dosyası indiriliyor...\")\n","    gdown.download(id=validation_file_id, output=zip_path, quiet=False)\n","    print(\"Dosyalar açılıyor...\")\n","    !unzip -q -o {zip_path} -d {dest_folder}\n","else:\n","    print(\"Doğrulama seti zaten mevcut.\")\n","\n","val_img_dir = os.path.join(dest_folder, 'Drone_TestSet')\n","val_xml_dir = os.path.join(dest_folder, 'Drone_TestSet_XMLs')\n","val_transform = A.Compose([A.Resize(480, 640), A.Normalize(), ToTensorV2()])\n","validation_dataset = DroneSegDataset(val_img_dir, val_xml_dir, transform=val_transform)\n","validation_loader = DataLoader(validation_dataset, batch_size=8, shuffle=False, num_workers=2)\n","print(f\"Doğrulama seti oluşturuldu: {len(validation_dataset)} örnek.\")\n","\n","# 2. En İyi Modeli Yükle ve Değerlendir\n","best_model_path = '/content/drive/MyDrive/hybrid_drone_model_with_vit_FULL.pth'\n","\n","if not os.path.exists(best_model_path):\n","    print(f\"\\n!!! HATA: En iyi model bulunamadı: {best_model_path}\")\n","else:\n","    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","    print(f\"\\nModel yükleniyor: {best_model_path}\")\n","\n","    best_model = torch.load(best_model_path, map_location=device, weights_only=False)\n","    best_model.eval()\n","\n","    print(\"Model başarıyla yüklendi.\")\n","    total_iou, total_dice, total_pixel_acc, num_batches = 0, 0, 0, len(validation_loader)\n","\n","    with torch.no_grad():\n","        for imgs, masks in tqdm(validation_loader, desc=\"Metrikler\"):\n","            imgs, masks = imgs.to(device), masks.to(device).float().unsqueeze(1)\n","\n","            preds = torch.sigmoid(best_model(imgs))\n","\n","            if preds.shape[-2:] != masks.shape[-2:]:\n","                preds = F.interpolate(preds, size=masks.shape[2:], mode='bilinear', align_corners=False)\n","\n","            preds_binary = (preds > 0.5).float()\n","\n","            intersection = torch.sum(preds_binary * masks)\n","            total_size = torch.sum(preds_binary) + torch.sum(masks)\n","            epsilon = 1e-6\n","\n","            total_dice += ((2. * intersection + epsilon) / (total_size + epsilon)).item()\n","            total_iou += ((intersection + epsilon) / (total_size - intersection + epsilon)).item()\n","            total_pixel_acc += (preds_binary == masks).sum().item() / masks.numel()\n","\n","    print(\"\\n--- ÖZEL DOĞRULAMA SETİ SONUÇLARI ---\")\n","    print(f\"Ortalama Piksel Doğruluğu: {total_pixel_acc / num_batches:.4f}\")\n","    print(f\"Ortalama IoU (Jaccard Index): {total_iou / num_batches:.4f}\")\n","    print(f\"Ortalama Dice Katsayısı: {total_dice / num_batches:.4f}\")\n","    print(\"--------------------------------------\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":344,"referenced_widgets":["9dde108b99494f58a9faf1782b3851f7","641031ca219b42f0870adc7b22d5bfdd","3554750b9c344899a4efd364301f1eef","c41d67c3e17645f6b96a8760d23e9177","f0e315c6b442455499c075f11cf59181","2cff54f8951447e88b0f73bacb763ebe","0ab87d6db6284ee18ec0d25bb25d4f12","542ae010d03b45f1b175d976073b3a92","f4d5610e2fd346749d594bd88b90b02e","383685041a5348afb3a56f6e64c9c4fd","7816902c185f4c1481efa3c88cc882df"]},"id":"8Fu3scr6KI4B","executionInfo":{"status":"ok","timestamp":1753440423144,"user_tz":-180,"elapsed":41245,"user":{"displayName":"Türker Efe Soyutemiz","userId":"11416081914510237351"}},"outputId":"7755322d-74d7-4d9e-e5a3-0906924574e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[">>> BÖLÜM 1: Ortam kuruluyor ve gerekli sınıflar tanımlanıyor...\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",">>> BÖLÜM 1: Kurulum ve tanımlamalar tamamlandı.\n","\n"," BÖLÜM 2: Özel doğrulama süreci \n","Doğrulama seti zaten mevcut.\n","Doğrulama seti oluşturuldu: 5375 örnek.\n","\n","Model yükleniyor: /content/drive/MyDrive/hybrid_drone_model_with_vit_FULL.pth\n","Model başarıyla yüklendi.\n"]},{"output_type":"display_data","data":{"text/plain":["Metrikler:   0%|          | 0/672 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","--- ÖZEL DOĞRULAMA SETİ SONUÇLARI ---\n","Ortalama Piksel Doğruluğu: 0.9744\n","Ortalama IoU (Jaccard Index): 0.3711\n","Ortalama Dice Katsayısı: 0.4301\n","--------------------------------------\n","\n"]}]},{"cell_type":"code","source":["#Validation Görselleri Üzerinde Maskeleme\n","\n","import os\n","import shutil\n","import gdown\n","from tqdm.auto import tqdm\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import cv2\n","import numpy as np\n","import xml.etree.ElementTree as ET\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from google.colab import drive\n","import matplotlib.pyplot as plt\n","import timm\n","\n","# Google Drive'ı bağla\n","drive.mount('/content/drive')\n","\n","# Veri Seti Sınıfı Tanımı\n","class DroneSegDataset(Dataset):\n","    def __init__(self, img_dir, xml_dir, transform=None):\n","        self.img_paths = sorted([os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.lower().endswith('.jpg')])\n","        self.xml_dir = xml_dir\n","        self.transform = transform\n","    def __len__(self): return len(self.img_paths)\n","    def __getitem__(self, idx):\n","        img_path = self.img_paths[idx]\n","        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n","        h, w = img.shape[:2]\n","        mask = np.zeros((h, w), dtype=np.uint8)\n","        xml_path = os.path.join(self.xml_dir, os.path.basename(img_path).replace('.jpg', '.xml'))\n","        if os.path.isfile(xml_path):\n","            tree = ET.parse(xml_path)\n","            for obj in tree.findall('object'):\n","                bbox = obj.find('bndbox')\n","                xmin, ymin = int(float(bbox.find('xmin').text)), int(float(bbox.find('ymin').text))\n","                xmax, ymax = int(float(bbox.find('xmax').text)), int(float(bbox.find('ymax').text))\n","                mask[ymin:ymax, xmin:xmax] = 1\n","        if self.transform:\n","            augmented = self.transform(image=img, mask=mask)\n","            img, mask = augmented['image'], augmented['mask']\n","        return img, mask\n","\n","# Model Mimarisi Sınıfı Tanımı\n","class HybridModelWithViT(nn.Module):\n","    def __init__(self, yolo_backbone, unet_encoder, cy, cu):\n","        super().__init__()\n","        self.yolo_backbone = yolo_backbone\n","        self.unet_encoder = unet_encoder\n","        self.fuse = nn.Conv2d(cy + cu, 512, kernel_size=1)\n","        self.vit_block = timm.create_model('vit_tiny_patch16_224', pretrained=True, in_chans=512, num_classes=0)\n","        self.decoder = nn.Conv2d(192, 512, kernel_size=1)\n","        self.head = nn.Conv2d(512, 1, kernel_size=1)\n","    def forward(self, x):\n","        f_y = self.yolo_backbone(x)\n","        f_u = self.unet_encoder(x)[-1]\n","        if f_u.shape[2:] != f_y.shape[2:]: f_u = F.interpolate(f_u, size=f_y.shape[2:], mode='bilinear', align_corners=False)\n","        m = torch.cat([f_y, f_u], dim=1)\n","        fused_features = self.fuse(m)\n","        vit_input = F.interpolate(fused_features, size=(224, 224), mode='bilinear', align_corners=False)\n","        vit_output = self.vit_block.forward_features(vit_input)\n","        B, _, C = vit_output.shape\n","        H_vit, W_vit = 14, 14\n","        vit_features_reshaped = vit_output[:, 1:, :].permute(0, 2, 1).reshape(B, C, H_vit, W_vit)\n","        original_H, original_W = fused_features.shape[2:]\n","        vit_features_downscaled = F.interpolate(vit_features_reshaped, size=(original_H, original_W), mode='bilinear', align_corners=False)\n","        decoded_vit_features = self.decoder(vit_features_downscaled)\n","        final_features = fused_features + decoded_vit_features\n","        return self.head(final_features)\n","\n","\n","print(\"Rastgele görsel değerlendirme süreci\")\n","\n","# Doğrulama Seti\n","validation_file_id = '1dJdUIqTfW7InN76Xb3flWVrVmcficP93'\n","dest_folder = '/content/VALIDATION'\n","zip_path = f\"{dest_folder}.zip\"\n","\n","if not os.path.exists(dest_folder):\n","    print(\"Doğrulama ZIP dosyası indiriliyor...\")\n","    gdown.download(id=validation_file_id, output=zip_path, quiet=False)\n","    print(\"Dosyalar açılıyor...\")\n","    !unzip -q -o {zip_path} -d {dest_folder}\n","else:\n","    print(\"Doğrulama seti zaten mevcut.\")\n","\n","val_img_dir = os.path.join(dest_folder, 'Drone_TestSet')\n","val_xml_dir = os.path.join(dest_folder, 'Drone_TestSet_XMLs')\n","val_transform = A.Compose([A.Resize(480, 640), A.Normalize(), ToTensorV2()])\n","validation_dataset = DroneSegDataset(val_img_dir, val_xml_dir, transform=val_transform)\n","\n","# Batch size, tek seferde 15 rastgele örnek\n","validation_loader = DataLoader(validation_dataset, batch_size=15, shuffle=True, num_workers=2)\n","print(f\"Rastgele örnekler için doğrulama seti oluşturuldu: {len(validation_dataset)} örnek.\")\n","\n","best_model_path = '/content/drive/MyDrive/hybrid_drone_model_with_vit_FULL.pth'\n","\n","if not os.path.exists(best_model_path):\n","    print(f\"\\n!!! HATA: En iyi model bulunamadı: {best_model_path}\")\n","else:\n","    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","    print(f\"\\nModel yükleniyor: {best_model_path}\")\n","\n","    best_model = torch.load(best_model_path, map_location=device, weights_only=False)\n","    best_model.eval()\n","\n","    print(\"Model başarıyla yüklendi. Rastgele 15 örnek üzerinde tahmin yapılıyor...\")\n","\n","\n","    def denormalize(tensor):\n","        mean, std = np.array([0.485, 0.456, 0.406]), np.array([0.229, 0.224, 0.225])\n","        img_np = tensor.cpu().numpy().transpose(1, 2, 0)\n","        return np.clip(std * img_np + mean, 0, 1)\n","\n","    # DataLoader'dan sadece bir batch (15 rastgele örnek) al\n","    images, ground_truths = next(iter(validation_loader))\n","\n","    with torch.no_grad():\n","        predictions_small = torch.sigmoid(best_model(images.to(device)))\n","        predictions = F.interpolate(predictions_small, size=ground_truths.shape[1:], mode='bilinear', align_corners=False)\n","        predictions_binary = predictions > 0.5\n","\n","\n","    num_samples = len(images)\n","    plt.figure(figsize=(15, num_samples * 5))\n","\n","    for i in range(num_samples):\n","\n","        plt.subplot(num_samples, 3, i * 3 + 1)\n","        plt.imshow(denormalize(images[i]))\n","        plt.title(f\"Giriş #{i+1}\")\n","        plt.axis('off')\n","\n","        # Gerçek Maske\n","        plt.subplot(num_samples, 3, i * 3 + 2)\n","        plt.imshow(ground_truths[i].squeeze(), cmap='gray')\n","        plt.title(f\"Gerçek Maske #{i+1}\")\n","        plt.axis('off')\n","\n","        # Tahmin (Bindirilmiş)\n","        plt.subplot(num_samples, 3, i * 3 + 3)\n","        original_image_np = (denormalize(images[i]) * 255).astype(np.uint8)\n","        predicted_mask_np = predictions_binary[i].squeeze().cpu().numpy().astype(np.uint8)\n","\n","        overlay = original_image_np.copy()\n","        overlay[predicted_mask_np == 1] = [0, 255, 0] # Yeşil renk\n","\n","        blended_image = cv2.addWeighted(original_image_np, 0.6, overlay, 0.4, 0)\n","\n","        plt.imshow(blended_image)\n","        plt.title(f\"Tahmin #{i+1}\")\n","        plt.axis('off')\n","\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1qchn52ZNQ26k2YGvZf_HoducYxcxcxOp"},"id":"TOZY7ZGyBJRC","executionInfo":{"status":"ok","timestamp":1753439738532,"user_tz":-180,"elapsed":15928,"user":{"displayName":"Türker Efe Soyutemiz","userId":"11416081914510237351"}},"outputId":"5162ab6e-248c-4a16-c286-4e717bd9d8e3"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["# Video üzerinde model kullanım hücresi\n","print(\">>> BÖLÜM 1: Ortam kuruluyor ve gerekli sınıflar tanımlanıyor...\")\n","\n","\n","%pip install --upgrade ultralytics segmentation-models-pytorch timm albumentations opencv-python matplotlib gdown tqdm -q\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import cv2\n","import numpy as np\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from google.colab import drive\n","import matplotlib.pyplot as plt\n","import timm\n","from ultralytics import YOLO\n","import segmentation_models_pytorch as smp\n","from tqdm.auto import tqdm\n","\n","drive.mount('/content/drive')\n","\n","# Sınıf Tanımları\n","class HybridModelWithViT(nn.Module):\n","    def __init__(self, yolo_backbone, unet_encoder, cy, cu):\n","        super().__init__()\n","        self.yolo_backbone = yolo_backbone\n","        self.unet_encoder = unet_encoder\n","        self.fuse = nn.Conv2d(cy + cu, 512, kernel_size=1)\n","        self.vit_block = timm.create_model('vit_tiny_patch16_224', pretrained=True, in_chans=512, num_classes=0)\n","        self.decoder = nn.Conv2d(192, 512, kernel_size=1)\n","        self.head = nn.Conv2d(512, 1, kernel_size=1)\n","    def forward(self, x):\n","        f_y = self.yolo_backbone(x)\n","        f_u = self.unet_encoder(x)[-1]\n","        if f_u.shape[2:] != f_y.shape[2:]: f_u = F.interpolate(f_u, size=f_y.shape[2:], mode='bilinear', align_corners=False)\n","        m = torch.cat([f_y, f_u], dim=1)\n","        fused_features = self.fuse(m)\n","        vit_input = F.interpolate(fused_features, size=(224, 224), mode='bilinear', align_corners=False)\n","        vit_output = self.vit_block.forward_features(vit_input)\n","        B, _, C = vit_output.shape\n","        H_vit, W_vit = 14, 14\n","        vit_features_reshaped = vit_output[:, 1:, :].permute(0, 2, 1).reshape(B, C, H_vit, W_vit)\n","        original_H, original_W = fused_features.shape[2:]\n","        vit_features_downscaled = F.interpolate(vit_features_reshaped, size=(original_H, original_W), mode='bilinear', align_corners=False)\n","        decoded_vit_features = self.decoder(vit_features_downscaled)\n","        final_features = fused_features + decoded_vit_features\n","        return self.head(final_features)\n","\n","print(\"BÖLÜM 1: Kurulum ve tanımlamalar tamamlandı.\\n\")\n","\n","\n","# VİDEO İŞLEME SÜRECİ\n","\n","print(\">>> BÖLÜM 2: Video işleme süreci başlatılıyor...\")\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","model_path = '/content/drive/MyDrive/hybrid_drone_model_with_vit_FULL.pth' # modelin yolu\n","\n","if not os.path.exists(model_path):\n","    print(f\"\\n!!! HATA: Model dosyası bulunamadı: {model_path}\")\n","else:\n","    print(f\"Model yükleniyor: {model_path}\")\n","    model = torch.load(model_path, map_location=device, weights_only=False)\n","    model.eval()\n","    print(\"Model yüklendi ve değerlendirme moduna alındı.\")\n","\n","    # Video Yolları\n","    input_video_path = '/content/drive/MyDrive/DRONEMURAT.mp4'\n","    output_video_path = '/content/drive/MyDrive/DRONEMURAT_MASKED.mp4'\n","\n","    # doğrulama seti ile aynı dönüşümler\n","    transform = A.Compose([\n","        A.Resize(480, 640),\n","        A.Normalize(),\n","        ToTensorV2()\n","    ])\n","\n","#Video Okuma ve Yazma Döngüsü\n","    cap = cv2.VideoCapture(input_video_path)\n","    if not cap.isOpened():\n","        print(f\"Hata: Video açılamadı -> {input_video_path}\")\n","    else:\n","        # Video özellikleri\n","        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","        fps = int(cap.get(cv2.CAP_PROP_FPS))\n","        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","        # Çıktı videosu için yazıcı\n","        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","        out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n","\n","        print(f\"\\nVideo işleniyor... Toplam {total_frames} kare.\")\n","\n","        # Her kareyi işlemek için döngü\n","        for _ in tqdm(range(total_frames), desc=\"Video Kareleri İşleniyor\"):\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","\n","            # A. Kareyi Hazırla\n","            img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            augmented = transform(image=img_rgb)\n","            img_tensor = augmented['image'].unsqueeze(0).to(device)\n","\n","            # B. Tahmin Yap\n","            with torch.no_grad():\n","                pred_small = torch.sigmoid(model(img_tensor))\n","                # Tahmini orijinal kare boyutuna büyüt\n","                pred_large = F.interpolate(pred_small, size=(height, width), mode='bilinear', align_corners=False)\n","                # Eşikleme ile ikili maske oluştur\n","                mask_binary = (pred_large > 0.5).squeeze().cpu().numpy().astype(np.uint8)\n","\n","            # C. Sonucu Görselleştir (Overlay)\n","            # Yeşil renkte bir maske katmanı\n","            green_mask_overlay = np.zeros_like(frame, dtype=np.uint8)\n","            green_mask_overlay[mask_binary == 1] = [0, 255, 0] # Yeşil\n","\n","            # Orijinal kare ile maskeyi birleştir\n","            blended_frame = cv2.addWeighted(frame, 0.7, green_mask_overlay, 0.3, 0)\n","\n","            # D. İşlenmiş Kareyi Çıktı Videosuna Yaz\n","            out.write(blended_frame)\n","\n","        cap.release()\n","        out.release()\n","        print(\"\\nİşlem tamamlandı!\")\n","        print(f\"Maskelenmiş video kaydedildi: {output_video_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":257,"referenced_widgets":["04990015fc4749d2beae8a3ef43b808f","c9ae0bc806d34360b5d10bd270b60b28","0a2ea6eee991427b9202c9801ee0d6f8","9cf4a159de5e497fbf907a2717131913","aa1ad659fcd142d5ba64a84f386aeab7","d6ef413a743a449db0e600dfb0e740bb","3840a9dfc4d64b979fee3da068a373ab","ac25edd5beb948929f38ccc27d9c6cfb","e53cc7b91d484c60ac7c38458d6c5784","345c967a82a74178bb1877965de1aaa5","871bc95200714eec8ff10b8de9705d38"]},"id":"gZ9Jc5qBM8xe","executionInfo":{"status":"ok","timestamp":1753441144949,"user_tz":-180,"elapsed":531504,"user":{"displayName":"Türker Efe Soyutemiz","userId":"11416081914510237351"}},"outputId":"e14f3c19-54d7-44d6-e245-887fa33fb5cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[">>> BÖLÜM 1: Ortam kuruluyor ve gerekli sınıflar tanımlanıyor...\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",">>> BÖLÜM 1: Kurulum ve tanımlamalar tamamlandı.\n","\n",">>> BÖLÜM 2: Video işleme süreci başlatılıyor...\n","Model yükleniyor: /content/drive/MyDrive/hybrid_drone_model_with_vit_FULL.pth\n","Model yüklendi ve değerlendirme moduna alındı.\n","\n","Video işleniyor... Toplam 9246 kare.\n"]},{"output_type":"display_data","data":{"text/plain":["Video Kareleri İşleniyor:   0%|          | 0/9246 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","İşlem tamamlandı!\n","Maskelenmiş video kaydedildi: /content/drive/MyDrive/DRONEMURAT_MASKED.mp4\n"]}]}]}